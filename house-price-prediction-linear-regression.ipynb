{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House price prediction data set\n\nConsisting of 4600 entries for homes in different US cities, this dataset includes 18 different variables that describe various property details such as size and location characteristics. \n\n**Columns:**\n\ndate: Date the house was sold<br>\nprice: Sale price of the house (Prediction target)<br>\nbedrooms: Number of the bedrooms<br>\nbathrooms: Number the bathrooms<br>\nsqft_living: Square footage of the home<br>\nsqft_lot: Square footage of the lot<br>\nfloors: Total floors (levels) in the house<br>\nwaterfront: House which has a view to a waterfront<br>\nview: Number of views<br>\ncondition: How good the condition is overall<br>\nsqft_above: Square footage of the house apart from basement<br>\nsqft_basement: Square footage of the basement<br>\nyr_built: Building year of the house<br>\nyr_renovated: Renovation year of the house<br>\nstreet: Street where the house is located<br>\ncity: City where the house is located<br>\nstatezip: Zip code of the region where the house is located<br>\ncountry: Country where the house is located (USA)<br>\n\n\nThere are 8 categorical (half nominal and half ordinal depending on the measurement level) and 9 discrete variables.\n\nOur ordinal variables, waterfront, view and and renovation year, appear numerically, but we will evaluate them in two categories, 0 and 1, depending on whether they are in the relevant house or not. Also, the condition has 5 unique values that increase from 1 to 5 based on the state of the house. And as you might guess, street, city, statezip, and country are the nominal ones.\n\n**Types of Data**\n1. Categorical (gender, blood type, aswers of yes/no questions, satisfaction/condition level etc.)\n2. Numerical<br>\na. Discrete (finite numbers- # of children/customer/room, physical money, time on a clock etc.)<br>\nb. Continuous (infinite numbers- measurements(weight, height), time in general etc.)<br>\n\n**Levels of Measurement**\n1. Qualitative<br>\na. Nominal (Can not be ordered- gender, blood type, seasons)<br>\nb. Ordinal (Takes values with an order or rank- satisfaction, condition level)<br>\n2. Quantitative<br>\na. Interval (Don't have a true zero- temperature, pH, IQ test etc.)<br>\nb. Ratios (Have a true zero- temperature only in Kelvin, amount, distance, pulse etc.)<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Relevant Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:22.333866Z","iopub.execute_input":"2022-03-24T13:52:22.334269Z","iopub.status.idle":"2022-03-24T13:52:22.930472Z","shell.execute_reply.started":"2022-03-24T13:52:22.33417Z","shell.execute_reply":"2022-03-24T13:52:22.929664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:22.93204Z","iopub.execute_input":"2022-03-24T13:52:22.932442Z","iopub.status.idle":"2022-03-24T13:52:22.936584Z","shell.execute_reply.started":"2022-03-24T13:52:22.932408Z","shell.execute_reply":"2022-03-24T13:52:22.935686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:22.937852Z","iopub.execute_input":"2022-03-24T13:52:22.938264Z","iopub.status.idle":"2022-03-24T13:52:23.008068Z","shell.execute_reply.started":"2022-03-24T13:52:22.938232Z","shell.execute_reply":"2022-03-24T13:52:23.007075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.010472Z","iopub.execute_input":"2022-03-24T13:52:23.010728Z","iopub.status.idle":"2022-03-24T13:52:23.245649Z","shell.execute_reply.started":"2022-03-24T13:52:23.010696Z","shell.execute_reply":"2022-03-24T13:52:23.244911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.246894Z","iopub.execute_input":"2022-03-24T13:52:23.247401Z","iopub.status.idle":"2022-03-24T13:52:23.252001Z","shell.execute_reply.started":"2022-03-24T13:52:23.24736Z","shell.execute_reply":"2022-03-24T13:52:23.251287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.253432Z","iopub.execute_input":"2022-03-24T13:52:23.253897Z","iopub.status.idle":"2022-03-24T13:52:23.285986Z","shell.execute_reply.started":"2022-03-24T13:52:23.253861Z","shell.execute_reply":"2022-03-24T13:52:23.284966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics as metrics","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.287334Z","iopub.execute_input":"2022-03-24T13:52:23.287569Z","iopub.status.idle":"2022-03-24T13:52:23.291902Z","shell.execute_reply.started":"2022-03-24T13:52:23.28754Z","shell.execute_reply":"2022-03-24T13:52:23.291073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.292973Z","iopub.execute_input":"2022-03-24T13:52:23.29319Z","iopub.status.idle":"2022-03-24T13:52:23.30602Z","shell.execute_reply.started":"2022-03-24T13:52:23.293165Z","shell.execute_reply":"2022-03-24T13:52:23.305007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Exploring the data set","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv(\"../input/housedata/data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.307137Z","iopub.execute_input":"2022-03-24T13:52:23.307939Z","iopub.status.idle":"2022-03-24T13:52:23.338451Z","shell.execute_reply.started":"2022-03-24T13:52:23.30789Z","shell.execute_reply":"2022-03-24T13:52:23.337461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick look to the data\n\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.342359Z","iopub.execute_input":"2022-03-24T13:52:23.342761Z","iopub.status.idle":"2022-03-24T13:52:23.374711Z","shell.execute_reply.started":"2022-03-24T13:52:23.342715Z","shell.execute_reply":"2022-03-24T13:52:23.374014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for missing values and data types of each column\n\nraw_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.376075Z","iopub.execute_input":"2022-03-24T13:52:23.376333Z","iopub.status.idle":"2022-03-24T13:52:23.394385Z","shell.execute_reply.started":"2022-03-24T13:52:23.376301Z","shell.execute_reply":"2022-03-24T13:52:23.393693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Added \"include=\"all\"\" parameter to see all columns. (to see not only numeric ones)\n# Based on the unique value count, city can be used by converting to dummies\n\nraw_data.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.395414Z","iopub.execute_input":"2022-03-24T13:52:23.395635Z","iopub.status.idle":"2022-03-24T13:52:23.458049Z","shell.execute_reply.started":"2022-03-24T13:52:23.395608Z","shell.execute_reply":"2022-03-24T13:52:23.457307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the accuracy of Linear regression without any data preprocessing\n\n# Define the variables\nx1 = raw_data.drop(['price', 'date', 'street', 'city', 'statezip', 'country'], axis = 1) \ny = raw_data['price']\n\n# Add a constant. Esentially, we are adding a new column (equal in lenght to x), which consists only of 1's\nx = sm.add_constant(x1)\n# Fit the model, according to the OLS (ordinary least squares) method with a dependent variable y and an independent x\nresults = sm.OLS(y,x).fit()\n# Print a summary of the model\nresults.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.459288Z","iopub.execute_input":"2022-03-24T13:52:23.459539Z","iopub.status.idle":"2022-03-24T13:52:23.524771Z","shell.execute_reply.started":"2022-03-24T13:52:23.459509Z","shell.execute_reply":"2022-03-24T13:52:23.523712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interpreting the Regression Table\n\nI used OLS (Ordinary least squares) model from statsmodel, which is often used in linear regression analysis. It compares the difference between individual points in our data set and the predicted best fit line to measure the amount of error produced.\n\nIn order to interpret this summary and to get an overview of the statistics here, we need to know the components in the table. I will try to explain the most important ones in my opinion.\n\nTypically when using statsmodels we have three main tables. The table called the model summary, which is at the top of the summary, contains information that is mostly known to us; such as **dependent variable, model, method, date and time we created the model, number of observations.\n\n**DF Residuals** is widely used in many contexts throughout statistics, including hypothesis testing, probability distributions, and regression analysis. It is represents the degrees of freedom of our model, the formula is given below.\n'(number of observations) - (number of predicting variables - 1)'<br>\n'(n-k-1 = 4600-11-1 = 4588)'. <br>\n\n\n**R-squared** is one of the most important explanatory power, a measure of how much of the independent variable is explained by changes in our dependent variables. R-squared 0 means our regression explains NONE of the variabilty of the data, while R-sguared equal 1 means it explains the entire variability. In our case, the model can only explain 21% of our dataset.\n\nChecking the **adjusted R-squared** is essential to analyze the effectiveness of multiple dependent variables on the model. Excessive use of variables is penalized when measuring, so while adding some variables that don't contribute to our model can increase our R-square, it often has a negative impact on the adjusted R-squared. It is always smaller than R squared.\n\nIn the second part of our summary, the table of coefficients, I want to focus on the coefficients of the **intercept** (the constant, they can be used interchangeably) first. This component expresses the biases of our model.(b0, b1 etc.) For each variable, it is the measure of how the change in that variable affects the independent one.\n\n**The standard error**, indicates the accuracy of our prediction for each variable. Lower the standard error better the estimate :)\n\nThere are also other values called **t statistics and its P-value**. The null hypothesis in this test is H0: B=0 (Is the coefficient equal to zero?) *P-value < 0.05 means that the variable is significant. 0.000 shows us size is a significant variable when predicting house prices!\n\nAs can be clearly seen from the explanations I have made so far, our model needs improvements. Let's start working on it!","metadata":{}},{"cell_type":"markdown","source":"# Dealing with missing values\n\nThere is no NaN value in our data set, but it would be good to check 0's or 999's for the sake of data consistency. I don't see any irrelevant max values in the describe output but there are some zeros to check.\n\nWe have to deal with 49 houses that look free, plus 2 houses each without a bedroom and a bathroom. Since they are few in number, we can remove these values directly from our data set, but since I want to try a few different methods, I will try to keep them by replacing them with appropriate values.\n\nI will leave waterfront as it is because zero values indicate there is no \"waterfront\" rather than the information being unknown. Same for: view, sqft_basement and yr_renovated. \n","metadata":{}},{"cell_type":"code","source":"raw_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.526374Z","iopub.execute_input":"2022-03-24T13:52:23.526898Z","iopub.status.idle":"2022-03-24T13:52:23.549424Z","shell.execute_reply.started":"2022-03-24T13:52:23.526852Z","shell.execute_reply":"2022-03-24T13:52:23.548483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data[raw_data==0].count()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.55102Z","iopub.execute_input":"2022-03-24T13:52:23.551513Z","iopub.status.idle":"2022-03-24T13:52:23.590789Z","shell.execute_reply.started":"2022-03-24T13:52:23.551469Z","shell.execute_reply":"2022-03-24T13:52:23.589821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_price = raw_data[raw_data['price']==0]\nzero_price.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.592513Z","iopub.execute_input":"2022-03-24T13:52:23.593031Z","iopub.status.idle":"2022-03-24T13:52:23.649794Z","shell.execute_reply.started":"2022-03-24T13:52:23.592988Z","shell.execute_reply":"2022-03-24T13:52:23.648894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In order to decide what to do with the null values in the price, \n# it is helpful to know the corelations between features.\n\ncorr_matrix = raw_data.corr()\n\nfig, ax = plt.subplots(figsize = (15,6))\nsns.heatmap(corr_matrix, annot = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:23.651466Z","iopub.execute_input":"2022-03-24T13:52:23.651989Z","iopub.status.idle":"2022-03-24T13:52:25.106084Z","shell.execute_reply.started":"2022-03-24T13:52:23.651939Z","shell.execute_reply":"2022-03-24T13:52:25.105036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# By looking at the median value of the sqft_living variable, which affects the price the most,\n# I divided the prices that appear 0 into two groups. \n# Afterwards, I decided on the value that I would assign to the empty prices in these two groups \n# by looking at the median values of the 3 variables that most affected the price value.\n\nlow_price_data = raw_data[(raw_data['sqft_living'] < zero_price['sqft_living'].median()) &\n         (raw_data['bathrooms'] < zero_price['bathrooms'].median()) &\n         (raw_data['sqft_above'] < zero_price['sqft_above'].median()) ]\nlow_price = low_price_data.price.median()\n\nhigh_price_data = raw_data[(raw_data['sqft_living'] > zero_price['sqft_living'].median()) &\n         (raw_data['bathrooms'] > zero_price['bathrooms'].median()) &\n         (raw_data['sqft_above'] > zero_price['sqft_above'].median()) ]\nhigh_price = high_price_data.price.median()\n\ndata_prc = raw_data.copy()\ndata_prc['price'] = np.where(((data_prc['price']==0) & (data_prc['sqft_living'] > zero_price['sqft_living'].median())), high_price, data_prc.price) \ndata_prc['price'] = np.where(((data_prc['price']==0) & (data_prc['sqft_living'] <= zero_price['sqft_living'].median())), low_price, data_prc.price)\n\ndata_prc.price[data_prc.price==0].count()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.108012Z","iopub.execute_input":"2022-03-24T13:52:25.108559Z","iopub.status.idle":"2022-03-24T13:52:25.135382Z","shell.execute_reply.started":"2022-03-24T13:52:25.108512Z","shell.execute_reply":"2022-03-24T13:52:25.134439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will print the distrubution plots to decide \n# which method to use fill in the unknown zero values in the bedrooms and batromms columns.\n# As you may notice, there is some skewness that will affect the mean of both features. \n# I will use the median imputation for replacing zero values.\n\nfig, ax = plt.subplots(1,2, figsize = (20,6))\n\nsns.distplot(ax = ax[0], x= data_prc.bedrooms, color='darkmagenta')\nax[0].set_title('Bedrooms', size = 18)\nsns.distplot(ax = ax[1], x = data_prc.bathrooms, color='darkmagenta')\nax[1].set_title('Bathrooms', size = 18)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.137033Z","iopub.execute_input":"2022-03-24T13:52:25.137444Z","iopub.status.idle":"2022-03-24T13:52:25.895596Z","shell.execute_reply.started":"2022-03-24T13:52:25.137396Z","shell.execute_reply":"2022-03-24T13:52:25.894621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_prc.groupby('bedrooms')[['bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']].mean()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.897085Z","iopub.execute_input":"2022-03-24T13:52:25.897382Z","iopub.status.idle":"2022-03-24T13:52:25.923181Z","shell.execute_reply.started":"2022-03-24T13:52:25.897339Z","shell.execute_reply":"2022-03-24T13:52:25.922515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_prc['bedrooms'] = data_prc['bedrooms'].replace(0, np.NaN)\ndata_prc['bedrooms'] = data_prc['bedrooms'].fillna(data_prc.bedrooms.median())\n\ndata_prc.bedrooms[data_prc.bedrooms==0].count()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.924156Z","iopub.execute_input":"2022-03-24T13:52:25.924838Z","iopub.status.idle":"2022-03-24T13:52:25.934551Z","shell.execute_reply.started":"2022-03-24T13:52:25.9248Z","shell.execute_reply":"2022-03-24T13:52:25.93361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_prc['bathrooms'].replace(to_replace = 0, value = data_prc.bathrooms.median(), inplace= True)\n\ndata_prc.bathrooms[data_prc.bathrooms==0].count()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.93581Z","iopub.execute_input":"2022-03-24T13:52:25.936169Z","iopub.status.idle":"2022-03-24T13:52:25.95085Z","shell.execute_reply.started":"2022-03-24T13:52:25.936133Z","shell.execute_reply":"2022-03-24T13:52:25.95002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_prc.groupby('bedrooms')[['bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']].mean()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.952408Z","iopub.execute_input":"2022-03-24T13:52:25.952686Z","iopub.status.idle":"2022-03-24T13:52:25.9845Z","shell.execute_reply.started":"2022-03-24T13:52:25.952653Z","shell.execute_reply":"2022-03-24T13:52:25.983853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with outliers\n\nOne of the most important parts of data preprocessing is detecting and treating the outliers as they affect our analysis and training process in a bad way.\n\nSimply, an outlier is an observation that lies far from the rest of the observations in a given dataset and they cause a skew. We can use some techniques such as boxplots, Z-score or Inter Quantile Range(IQR) to detect outliers. \n\nAfter determining these values, the next step is to decide what to do with these values. We can remove these values, use either method quarter-based flooring and covering, or mean/median imputation.","metadata":{}},{"cell_type":"code","source":"# A great step in the data exploration is to display the boxplot to see outliers\n\nsns.catplot(x='price', data=data_prc, kind='box', height=3, aspect=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:25.985621Z","iopub.execute_input":"2022-03-24T13:52:25.986051Z","iopub.status.idle":"2022-03-24T13:52:26.398727Z","shell.execute_reply.started":"2022-03-24T13:52:25.986Z","shell.execute_reply":"2022-03-24T13:52:26.398004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Also we can print the probability distribution function (PDF) of a variable\n# The PDF will show us how that variable is distributed \n# This makes it very easy to spot anomalies, icluded outliers\n# The PDF is often the basis on which we decide whether we want to transform a feature\n\nsns.distplot(data_prc.price, color='darkmagenta')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:26.400623Z","iopub.execute_input":"2022-03-24T13:52:26.401714Z","iopub.status.idle":"2022-03-24T13:52:26.825881Z","shell.execute_reply.started":"2022-03-24T13:52:26.401643Z","shell.execute_reply":"2022-03-24T13:52:26.82498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will use the IQR measurement for removing outliers.\n\nQ75 = np.percentile(data_prc['price'],75)\nQ25 = np.percentile(data_prc['price'],25)\nIQR = Q75-Q25\ncutoff = IQR * 1.5\nupper = Q75 + cutoff\nlower = 1\n\ndata1 = data_prc[(data_prc['price']<upper)]\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:26.827577Z","iopub.execute_input":"2022-03-24T13:52:26.827815Z","iopub.status.idle":"2022-03-24T13:52:26.836436Z","shell.execute_reply.started":"2022-03-24T13:52:26.827785Z","shell.execute_reply":"2022-03-24T13:52:26.83568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What a change!\n\nsns.distplot(data1.price, color='darkmagenta') \n\nprint(data_prc['price'].skew(),',', data1['price'].skew())","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:26.837638Z","iopub.execute_input":"2022-03-24T13:52:26.83841Z","iopub.status.idle":"2022-03-24T13:52:27.240163Z","shell.execute_reply.started":"2022-03-24T13:52:26.838369Z","shell.execute_reply":"2022-03-24T13:52:27.23913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:27.246121Z","iopub.execute_input":"2022-03-24T13:52:27.246472Z","iopub.status.idle":"2022-03-24T13:52:27.254535Z","shell.execute_reply.started":"2022-03-24T13:52:27.246422Z","shell.execute_reply":"2022-03-24T13:52:27.253171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look other variables.\n\ncols = ['bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n\nfig, ax = plt.subplots(4,3, figsize = (30,15))\n\nfor idx, col in enumerate(cols):\n    rn = math.floor(idx/3)\n    cn = idx%3\n    sns.distplot(ax=ax[rn,cn], x=data1[col], color='darkmagenta')\n    ax[rn,cn].set_title(col, size=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:27.256389Z","iopub.execute_input":"2022-03-24T13:52:27.256744Z","iopub.status.idle":"2022-03-24T13:52:31.296603Z","shell.execute_reply.started":"2022-03-24T13:52:27.256697Z","shell.execute_reply":"2022-03-24T13:52:31.29591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bedrooms:\nprint(data1.bedrooms.value_counts().sort_index())\nsns.catplot(x='bedrooms', y='price', data=data1, height=3, aspect=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:31.297529Z","iopub.execute_input":"2022-03-24T13:52:31.297735Z","iopub.status.idle":"2022-03-24T13:52:31.812857Z","shell.execute_reply.started":"2022-03-24T13:52:31.297708Z","shell.execute_reply":"2022-03-24T13:52:31.811979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of houses with 7,8 and 9 bedrooms seems very low. \n# I will subtract these values\n\ndata2 = data1[data1.bedrooms<7]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:31.814118Z","iopub.execute_input":"2022-03-24T13:52:31.814361Z","iopub.status.idle":"2022-03-24T13:52:31.820661Z","shell.execute_reply.started":"2022-03-24T13:52:31.81433Z","shell.execute_reply":"2022-03-24T13:52:31.819821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I could not understand the meaning of 0.75 or 1.25 bathroom houses. \n# I'll first convert their types to integers.\n\ndata2.bathrooms.value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:31.821845Z","iopub.execute_input":"2022-03-24T13:52:31.822167Z","iopub.status.idle":"2022-03-24T13:52:31.838434Z","shell.execute_reply.started":"2022-03-24T13:52:31.822134Z","shell.execute_reply":"2022-03-24T13:52:31.837535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.bathrooms = data2.bathrooms.astype(int)\nprint(data2.bathrooms.value_counts().sort_index())\n\nsns.catplot(x='bathrooms', y='price', data=data2, height=3, aspect=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:31.839875Z","iopub.execute_input":"2022-03-24T13:52:31.840838Z","iopub.status.idle":"2022-03-24T13:52:32.302016Z","shell.execute_reply.started":"2022-03-24T13:52:31.840789Z","shell.execute_reply":"2022-03-24T13:52:32.300872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = data2[data2.bathrooms<4]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:32.303597Z","iopub.execute_input":"2022-03-24T13:52:32.304411Z","iopub.status.idle":"2022-03-24T13:52:32.310975Z","shell.execute_reply.started":"2022-03-24T13:52:32.304366Z","shell.execute_reply":"2022-03-24T13:52:32.310068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q = data3.sqft_living.quantile(0.99)\ndata4 = data3[data3.sqft_living<q]\nprint(data3.sqft_living.skew(),',', data4.sqft_living.skew())","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:32.31208Z","iopub.execute_input":"2022-03-24T13:52:32.31271Z","iopub.status.idle":"2022-03-24T13:52:32.32956Z","shell.execute_reply.started":"2022-03-24T13:52:32.312676Z","shell.execute_reply":"2022-03-24T13:52:32.328852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There is still some skew\n# But when we look at the price versus scatter plot, it looks much better now.\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,3))\n\nax1.scatter(x= 'sqft_living', y= 'price', data = data3, c= 'sqft_living', cmap='inferno')\nax1.set_title('With Outliers')\nax1.set_xlim(0,7000)\nax1.set_ylim(-0.1e6,1.3e6)\nax2.scatter(x= 'sqft_living', y= 'price', data = data4, c= 'sqft_living', cmap='inferno')\nax2.set_title('Without Outliers')\nax2.set_xlim(0,7000)\nax2.set_ylim(-0.1e6,1.3e6)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:32.330639Z","iopub.execute_input":"2022-03-24T13:52:32.331086Z","iopub.status.idle":"2022-03-24T13:52:33.15101Z","shell.execute_reply.started":"2022-03-24T13:52:32.331048Z","shell.execute_reply":"2022-03-24T13:52:33.150215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q = data4.sqft_lot.quantile(0.99)\ndata5 = data4[data4.sqft_lot<q]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.152328Z","iopub.execute_input":"2022-03-24T13:52:33.153051Z","iopub.status.idle":"2022-03-24T13:52:33.160773Z","shell.execute_reply.started":"2022-03-24T13:52:33.153012Z","shell.execute_reply":"2022-03-24T13:52:33.159658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's compare the boxplots this time, there is a visible improvement.\n\nfig, ax = plt.subplots(1,2, figsize=(10,6))\n\nsns.boxplot(ax=ax[0], x=data4.sqft_lot, color = 'darkmagenta')\nax[0].set_title('With Outliers')\nsns.boxplot(ax=ax[1], x=data5.sqft_lot, color = 'darkmagenta')\nax[1].set_title('Without Outliers')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.162323Z","iopub.execute_input":"2022-03-24T13:52:33.162541Z","iopub.status.idle":"2022-03-24T13:52:33.522767Z","shell.execute_reply.started":"2022-03-24T13:52:33.162516Z","shell.execute_reply":"2022-03-24T13:52:33.52174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.floors.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.524263Z","iopub.execute_input":"2022-03-24T13:52:33.52458Z","iopub.status.idle":"2022-03-24T13:52:33.534098Z","shell.execute_reply.started":"2022-03-24T13:52:33.524547Z","shell.execute_reply":"2022-03-24T13:52:33.533084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again, I will convert the data type to integer.\n\ndata6 = data5.copy()\ndata6.floors = data6.floors.astype(int)\nprint(data6.floors.value_counts())\n\nsns.catplot(x='floors', y='price', data=data6, height=3, aspect=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.535476Z","iopub.execute_input":"2022-03-24T13:52:33.535734Z","iopub.status.idle":"2022-03-24T13:52:33.842507Z","shell.execute_reply.started":"2022-03-24T13:52:33.535701Z","shell.execute_reply":"2022-03-24T13:52:33.841892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As I said at the beginning, Waterfront is divided into two categories, I will keep it that way.\n\ndata6.waterfront.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.843475Z","iopub.execute_input":"2022-03-24T13:52:33.844273Z","iopub.status.idle":"2022-03-24T13:52:33.852907Z","shell.execute_reply.started":"2022-03-24T13:52:33.844223Z","shell.execute_reply":"2022-03-24T13:52:33.852114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since the number of house with a view is very few, it will be more useful for our analysis to see this feature as 0 and 1. \n\nprint(data6.view.value_counts())\n\ndata7=data6.copy()\ndata7.view = data7.view.map({0:0, 1:1, 2:1, 3:1, 4:1})\nprint(data7.view.value_counts())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.854582Z","iopub.execute_input":"2022-03-24T13:52:33.854791Z","iopub.status.idle":"2022-03-24T13:52:33.868661Z","shell.execute_reply.started":"2022-03-24T13:52:33.854765Z","shell.execute_reply":"2022-03-24T13:52:33.867632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There doesn't seem to be many homes in bad shape in the US, I'll just remove 1's.\n\nprint(data7.condition.value_counts())\n\ndata7 = data7[data7['condition']>1]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.869901Z","iopub.execute_input":"2022-03-24T13:52:33.870855Z","iopub.status.idle":"2022-03-24T13:52:33.877764Z","shell.execute_reply.started":"2022-03-24T13:52:33.870819Z","shell.execute_reply":"2022-03-24T13:52:33.877206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will keep sqft_above as is.\n\ndata8 = data7.copy()\nsns.boxplot(data7.sqft_above, color= 'darkmagenta')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:33.878788Z","iopub.execute_input":"2022-03-24T13:52:33.879477Z","iopub.status.idle":"2022-03-24T13:52:34.080494Z","shell.execute_reply.started":"2022-03-24T13:52:33.87944Z","shell.execute_reply":"2022-03-24T13:52:34.079504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I think removing 1% would be enough.\n\nq = data8.sqft_basement.quantile(0.99)\ndata9 = data8[data8.sqft_basement<q]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:34.082075Z","iopub.execute_input":"2022-03-24T13:52:34.082397Z","iopub.status.idle":"2022-03-24T13:52:34.093445Z","shell.execute_reply.started":"2022-03-24T13:52:34.082351Z","shell.execute_reply":"2022-03-24T13:52:34.092521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are many houses that do not have a basement.\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,3))\n\nax1.scatter(x= 'sqft_basement', y= 'price', data = data8, c= 'sqft_basement', cmap='inferno')\nax1.set_title('With Outliers')\nax1.set_xlim(-100,2500)\nax1.set_ylim(-0.1e6,1.25e6)\nax2.scatter(x= 'sqft_basement', y= 'price', data = data9, c= 'sqft_basement', cmap='inferno')\nax2.set_title('Without Outliers')\nax2.set_xlim(-100,2500)\nax2.set_ylim(-0.1e6,1.25e6)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:34.095169Z","iopub.execute_input":"2022-03-24T13:52:34.09575Z","iopub.status.idle":"2022-03-24T13:52:34.806446Z","shell.execute_reply.started":"2022-03-24T13:52:34.095703Z","shell.execute_reply":"2022-03-24T13:52:34.805355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will keep building year as is.\n\nsns.boxplot(data9.yr_built, color= 'darkmagenta')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:34.808282Z","iopub.execute_input":"2022-03-24T13:52:34.808613Z","iopub.status.idle":"2022-03-24T13:52:34.993746Z","shell.execute_reply.started":"2022-03-24T13:52:34.808556Z","shell.execute_reply":"2022-03-24T13:52:34.992976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the last feature I will take as renovated(1) not (0).\n\nsns.distplot(data9.yr_renovated, color= 'darkmagenta')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:34.994842Z","iopub.execute_input":"2022-03-24T13:52:34.995092Z","iopub.status.idle":"2022-03-24T13:52:35.264607Z","shell.execute_reply.started":"2022-03-24T13:52:34.995064Z","shell.execute_reply":"2022-03-24T13:52:35.263757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data9.yr_renovated = pd.np.where(data9.yr_renovated==0,0,1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.266415Z","iopub.execute_input":"2022-03-24T13:52:35.267004Z","iopub.status.idle":"2022-03-24T13:52:35.273432Z","shell.execute_reply.started":"2022-03-24T13:52:35.26695Z","shell.execute_reply":"2022-03-24T13:52:35.272618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We've removed most of the outliers. First I'll just continue with numeric values.\n# Let's continue by dropping the categorical variables and saving it as a separate data set.\n\ndata_pp = data9.drop(['date', 'city', 'street', 'statezip', 'country'], axis=1)\ndata_pp.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.274537Z","iopub.execute_input":"2022-03-24T13:52:35.27489Z","iopub.status.idle":"2022-03-24T13:52:35.333673Z","shell.execute_reply.started":"2022-03-24T13:52:35.274859Z","shell.execute_reply":"2022-03-24T13:52:35.332657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the data\n\nScaling is transforming our data to fit on a specific scale like 0-100 or 0-1. It is critical when using metrics-based methods of how far away data points are, such as support vector machines (SVM) or k-nearest neighbors (KNN), rather than linear regression. However, I still think it will be useful to scale the data, I will use the Robust Scaler as there is still some outlier in our data.","metadata":{}},{"cell_type":"code","source":"data_pp = data_pp.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.335145Z","iopub.execute_input":"2022-03-24T13:52:35.335366Z","iopub.status.idle":"2022-03-24T13:52:35.339223Z","shell.execute_reply.started":"2022-03-24T13:52:35.335338Z","shell.execute_reply":"2022-03-24T13:52:35.338693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the targets and inputs.\n\ntargets = data_pp.iloc[:,:1]\nunscaled_inputs = data_pp.drop(['price'], axis = 1)\n\nunscaled_inputs.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.340494Z","iopub.execute_input":"2022-03-24T13:52:35.340725Z","iopub.status.idle":"2022-03-24T13:52:35.36305Z","shell.execute_reply.started":"2022-03-24T13:52:35.340697Z","shell.execute_reply":"2022-03-24T13:52:35.362029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will divide our data into train and test groups. This is important to avoid overfitting or underfitting.\n# Overfitting means, our training has focused on the particular training data set so much, so it has missed the point. \n# Underfitting means the model has not captured the underlying logic of the data.\n\nx_train, x_test, y_train, y_test = train_test_split(unscaled_inputs, targets, test_size=0.2, random_state=42)\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.364335Z","iopub.execute_input":"2022-03-24T13:52:35.364738Z","iopub.status.idle":"2022-03-24T13:52:35.377011Z","shell.execute_reply.started":"2022-03-24T13:52:35.364698Z","shell.execute_reply":"2022-03-24T13:52:35.376074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will keep features that contain only 0 or 1 data separately.\n\ncolumns_to_scale = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n        'condition', 'sqft_above', 'sqft_basement',\n       'yr_built']\ncolumns_to_omit = ['waterfront','view','yr_renovated']\nx_train_to_scale = x_train[columns_to_scale]\nx_test_to_scale = x_test[columns_to_scale]\nx_train_not_to_scale = x_train[columns_to_omit].reset_index(drop=True)\nx_test_not_to_scale = x_test[columns_to_omit].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.378428Z","iopub.execute_input":"2022-03-24T13:52:35.37874Z","iopub.status.idle":"2022-03-24T13:52:35.389853Z","shell.execute_reply.started":"2022-03-24T13:52:35.378699Z","shell.execute_reply":"2022-03-24T13:52:35.388994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's fit the scaler using only the training data, then transform both training and test data.\n\nscaler = RobustScaler()  #StandardScaler()\nscaler.fit(x_train_to_scale)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.391286Z","iopub.execute_input":"2022-03-24T13:52:35.39178Z","iopub.status.idle":"2022-03-24T13:52:35.410645Z","shell.execute_reply.started":"2022-03-24T13:52:35.391733Z","shell.execute_reply":"2022-03-24T13:52:35.409741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_scaled = scaler.transform(x_train_to_scale)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.412418Z","iopub.execute_input":"2022-03-24T13:52:35.412888Z","iopub.status.idle":"2022-03-24T13:52:35.419992Z","shell.execute_reply.started":"2022-03-24T13:52:35.412846Z","shell.execute_reply":"2022-03-24T13:52:35.419361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_scaled = scaler.transform(x_test_to_scale)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.42103Z","iopub.execute_input":"2022-03-24T13:52:35.421739Z","iopub.status.idle":"2022-03-24T13:52:35.432802Z","shell.execute_reply.started":"2022-03-24T13:52:35.421702Z","shell.execute_reply":"2022-03-24T13:52:35.432017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After scaling our training and test data are converted to np.arrays,\n# Let's make them pd.DataFrame again and merge them with unscaled features.\n\nx_train_scaled = pd.DataFrame(x_train_scaled, columns=columns_to_scale)\nx_train = pd.concat([x_train_scaled,x_train_not_to_scale], axis=1)\ny_train = y_train.reset_index(drop=True)\n\nx_test_scaled = pd.DataFrame(x_test_scaled, columns=columns_to_scale)\nx_test = pd.concat([x_test_scaled,x_test_not_to_scale], axis=1)\ny_test = y_test.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.434095Z","iopub.execute_input":"2022-03-24T13:52:35.434701Z","iopub.status.idle":"2022-03-24T13:52:35.444459Z","shell.execute_reply.started":"2022-03-24T13:52:35.434662Z","shell.execute_reply":"2022-03-24T13:52:35.443627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we are ready!\n\nx_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.445998Z","iopub.execute_input":"2022-03-24T13:52:35.44679Z","iopub.status.idle":"2022-03-24T13:52:35.467187Z","shell.execute_reply.started":"2022-03-24T13:52:35.446746Z","shell.execute_reply":"2022-03-24T13:52:35.46647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model - Linear Regression","metadata":{}},{"cell_type":"code","source":"# Let's create the model as we did at the beginning.\n\nx = sm.add_constant(x_train)\nresults = sm.OLS(y_train, x).fit()\n\nresults.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.470358Z","iopub.execute_input":"2022-03-24T13:52:35.470639Z","iopub.status.idle":"2022-03-24T13:52:35.51357Z","shell.execute_reply.started":"2022-03-24T13:52:35.470608Z","shell.execute_reply":"2022-03-24T13:52:35.51266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We talked about the low contribution of variables with P values above 0.05 to the model, \n# so I'm going to drop the renovation year value and run the model again.\n\nx_train_pv = x_train.drop(['yr_renovated'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.515209Z","iopub.execute_input":"2022-03-24T13:52:35.515714Z","iopub.status.idle":"2022-03-24T13:52:35.525152Z","shell.execute_reply.started":"2022-03-24T13:52:35.51567Z","shell.execute_reply":"2022-03-24T13:52:35.524085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R2 and Adj. R2 is the same as before. \n# We did well by removing it out, because it's always better to keep the equation simple.\n\nX = sm.add_constant(x_train_pv)\nresults = sm.OLS(y_train, X).fit()\n\nresults.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.526782Z","iopub.execute_input":"2022-03-24T13:52:35.527297Z","iopub.status.idle":"2022-03-24T13:52:35.580763Z","shell.execute_reply.started":"2022-03-24T13:52:35.527253Z","shell.execute_reply":"2022-03-24T13:52:35.579857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding the Categorical Variables\n\nWe have developed our model quite well, but I would like to enrich it a bit more by adding some categorical variables. To do this we need to transform categorical data into appropriate numeric values, there is no single method how to approach this issue. Fortunately, python tools of pandas and scikit-learn provide several approaches that can be implemented. \n\nHere some methods that can be used:\n\n**One-Hot Encoding**\nOne-Hot Encoding gets each group of any categorical variable a new variable by mapping each group to binary numbers (0 or 1). Used when data is nominal. Newly created binary properties can be considered as dummy variables.<br>\n**Dummy Encoding**\nThe Dummy encoding scheme is similar to one-hot encoding. In the case of one-hot encoding, it uses N binary variables for N categories in a variable. Dummy encoding is a minor improvement over one-hot encoding, it uses N-1 properties to represent N tags/categories.<br>\n**Effect Encoding**\nWhen applying this encoding type, the categories are given values in the format -1,0,1. The -1 occurrence is the only difference between One-Hot encoding and effects encoding.<br>\n**Label Encoding or Ordinal Encoding**\nThis type of encoding is used when the variables in the data are ordinal. It converts each label to integer values, and the encoded data represents the label array.","metadata":{}},{"cell_type":"code","source":"data_wd = data9.drop(['date', 'street', 'statezip', 'country'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.582399Z","iopub.execute_input":"2022-03-24T13:52:35.582906Z","iopub.status.idle":"2022-03-24T13:52:35.592899Z","shell.execute_reply.started":"2022-03-24T13:52:35.58286Z","shell.execute_reply":"2022-03-24T13:52:35.591901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get_dummies is one of the common ways to create dummy variables for categorical features\n\ncity_dummies = pd.get_dummies(data_wd.city, drop_first = True)\ncity_dummies","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.594743Z","iopub.execute_input":"2022-03-24T13:52:35.595276Z","iopub.status.idle":"2022-03-24T13:52:35.637952Z","shell.execute_reply.started":"2022-03-24T13:52:35.595228Z","shell.execute_reply":"2022-03-24T13:52:35.637068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's define the variables one more time.\n\ntargets = data_wd.iloc[:,:1]\n\na = data_wd.drop(['price','city'], axis = 1)\nunscaled_inputs_wd = pd.concat([a, city_dummies], axis=1)\nunscaled_inputs_wd.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.639559Z","iopub.execute_input":"2022-03-24T13:52:35.640058Z","iopub.status.idle":"2022-03-24T13:52:35.675781Z","shell.execute_reply.started":"2022-03-24T13:52:35.640013Z","shell.execute_reply":"2022-03-24T13:52:35.674934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the targets and inputs into train-test data again.\n\nx_train, x_test, y_train, y_test = train_test_split(unscaled_inputs_wd, targets, test_size=0.2, random_state = 42)\n\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.677525Z","iopub.execute_input":"2022-03-24T13:52:35.677868Z","iopub.status.idle":"2022-03-24T13:52:35.690913Z","shell.execute_reply.started":"2022-03-24T13:52:35.677821Z","shell.execute_reply":"2022-03-24T13:52:35.690002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.692763Z","iopub.execute_input":"2022-03-24T13:52:35.693395Z","iopub.status.idle":"2022-03-24T13:52:35.701262Z","shell.execute_reply.started":"2022-03-24T13:52:35.693344Z","shell.execute_reply":"2022-03-24T13:52:35.700272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_scale = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'condition', 'sqft_above', 'sqft_basement',\n       'yr_built']\ncolumns_to_omit = ['waterfront','view','yr_renovated', 'Auburn', 'Beaux Arts Village',\n       'Bellevue', 'Black Diamond', 'Bothell', 'Burien', 'Carnation',\n       'Clyde Hill', 'Covington', 'Des Moines', 'Duvall', 'Enumclaw',\n       'Fall City', 'Federal Way', 'Inglewood-Finn Hill', 'Issaquah',\n       'Kenmore', 'Kent', 'Kirkland', 'Lake Forest Park', 'Maple Valley',\n       'Medina', 'Mercer Island', 'Milton', 'Newcastle', 'Normandy Park',\n       'North Bend', 'Pacific', 'Preston', 'Ravensdale', 'Redmond',\n       'Renton', 'Sammamish', 'SeaTac', 'Seattle', 'Shoreline',\n       'Skykomish', 'Snoqualmie', 'Snoqualmie Pass', 'Tukwila', 'Vashon',\n       'Woodinville', 'Yarrow Point']\nx_train_to_scale = x_train[columns_to_scale]\nx_test_to_scale = x_test[columns_to_scale]\nx_train_not_to_scale = x_train[columns_to_omit].reset_index(drop=True)\nx_test_not_to_scale = x_test[columns_to_omit].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.702895Z","iopub.execute_input":"2022-03-24T13:52:35.703523Z","iopub.status.idle":"2022-03-24T13:52:35.719672Z","shell.execute_reply.started":"2022-03-24T13:52:35.703483Z","shell.execute_reply":"2022-03-24T13:52:35.718767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale as before\n\nscaler = RobustScaler()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.721148Z","iopub.execute_input":"2022-03-24T13:52:35.72182Z","iopub.status.idle":"2022-03-24T13:52:35.726614Z","shell.execute_reply.started":"2022-03-24T13:52:35.721768Z","shell.execute_reply":"2022-03-24T13:52:35.725988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler.fit(x_train_to_scale)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.727934Z","iopub.execute_input":"2022-03-24T13:52:35.728699Z","iopub.status.idle":"2022-03-24T13:52:35.745511Z","shell.execute_reply.started":"2022-03-24T13:52:35.728659Z","shell.execute_reply":"2022-03-24T13:52:35.744576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_scaled = scaler.transform(x_train_to_scale)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.746974Z","iopub.execute_input":"2022-03-24T13:52:35.747443Z","iopub.status.idle":"2022-03-24T13:52:35.755602Z","shell.execute_reply.started":"2022-03-24T13:52:35.74741Z","shell.execute_reply":"2022-03-24T13:52:35.753588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_scaled = scaler.transform(x_test_to_scale)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.758256Z","iopub.execute_input":"2022-03-24T13:52:35.758737Z","iopub.status.idle":"2022-03-24T13:52:35.769362Z","shell.execute_reply.started":"2022-03-24T13:52:35.758697Z","shell.execute_reply":"2022-03-24T13:52:35.768191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_scaled = pd.DataFrame(x_train_scaled, columns=columns_to_scale)\nx_train = pd.concat([x_train_scaled,x_train_not_to_scale], axis=1)\ny_train = y_train.reset_index(drop=True)\n\nx_test_scaled = pd.DataFrame(x_test_scaled, columns=columns_to_scale)\nx_test = pd.concat([x_test_scaled,x_test_not_to_scale], axis=1)\ny_test = y_test.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.77097Z","iopub.execute_input":"2022-03-24T13:52:35.771557Z","iopub.status.idle":"2022-03-24T13:52:35.784262Z","shell.execute_reply.started":"2022-03-24T13:52:35.771512Z","shell.execute_reply":"2022-03-24T13:52:35.783452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model - Linear Regression","metadata":{}},{"cell_type":"code","source":"# Yey! We managed to get much better results now.\n# 'sqft_lot' and 'yr_renovated' features seems insignificant when we evaluate them according to their p-values.\n\nX1 = sm.add_constant(x_train)\nresults = sm.OLS(y_train, X1).fit()\n\nresults.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.785385Z","iopub.execute_input":"2022-03-24T13:52:35.786042Z","iopub.status.idle":"2022-03-24T13:52:35.921637Z","shell.execute_reply.started":"2022-03-24T13:52:35.786005Z","shell.execute_reply":"2022-03-24T13:52:35.920574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.923833Z","iopub.execute_input":"2022-03-24T13:52:35.92458Z","iopub.status.idle":"2022-03-24T13:52:35.93383Z","shell.execute_reply.started":"2022-03-24T13:52:35.924524Z","shell.execute_reply":"2022-03-24T13:52:35.932763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assumption check for Lineer Regression\n\n**1)Linearity** In the name says it all, we need to see a linear relationship between independent and dependent variables. We can basically determine this using scatter plots. It was also important to check for outliers since linear regression is sensitive to outlier effects. To fix: Run a non-linear regression, exponential transformation, log transformation\n\n**2)No Endogeneity:** Omitted variable bias occurs when we forget to include a variable. This is reflected in the error term as the factor you forgot about is included in the error. In this way, the error is not random but includes a systematic part (the omitted variable).\n\n**3)Normality and Homoscedasticy:** We have assumed that the errors have normal distribution, Zero Mean: Having an intercept solves that problem, in real life it is unusual to having that problem. Homoscedasticity: It means to having equal variance. To fix: Look for OVB, Look for outliers, Apply log transformation\n\n**4)No Autocorrelation(no serial correlation):** Durbin Watson score falls between zero and four. <1 and >3 cause an alarm, if we detect the problem, we need to use alternative method.\n\n**5)No Multicollinearity:** Independent variables that effect each others. To Fix= Drop one of the two variables, transform into one new variable.","metadata":{}},{"cell_type":"code","source":"# Let's check if there are multicollunearity.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# all features where we want to check for multicollinearity:\nvariables = x_train[[ 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'waterfront', 'condition',  'sqft_basement',\n       'yr_built', 'view', 'yr_renovated', 'Auburn', 'Beaux Arts Village',\n       'Bellevue', 'Black Diamond', 'Bothell', 'Burien', 'Carnation',\n       'Clyde Hill', 'Covington', 'Des Moines', 'Duvall', 'Enumclaw',\n       'Fall City', 'Federal Way', 'Inglewood-Finn Hill', 'Issaquah',\n       'Kenmore', 'Kent', 'Kirkland', 'Lake Forest Park', 'Maple Valley',\n       'Medina', 'Mercer Island', 'Milton', 'Newcastle', 'Normandy Park',\n       'North Bend', 'Pacific', 'Preston', 'Ravensdale', 'Redmond',\n       'Renton', 'Sammamish', 'SeaTac', 'Seattle', 'Shoreline',\n       'Skykomish', 'Snoqualmie', 'Snoqualmie Pass', 'Tukwila', 'Vashon',\n       'Woodinville', 'Yarrow Point']]\n\n# we create a new data frame which will include all the VIFs, \n# each variable has its own VIF as this measure is variable specific (not model specific)\nvif = pd.DataFrame()\n\n# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n\n# Finally, I will include names so it is easier to explore the result\nvif[\"Features\"] = variables.columns\n\nvif.sort_values(by='VIF')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:35.936221Z","iopub.execute_input":"2022-03-24T13:52:35.936893Z","iopub.status.idle":"2022-03-24T13:52:37.263793Z","shell.execute_reply.started":"2022-03-24T13:52:35.936832Z","shell.execute_reply":"2022-03-24T13:52:37.262864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will drop 'sqft_above' based on features VIF scores, \n# also 'sqft_lot' and 'yr_renovated' based on p-value that we have already determined.\n\nx_train = x_train.drop(['sqft_above','sqft_lot','yr_renovated'], axis=1)\nx_test = x_test.drop(['sqft_above','sqft_lot','yr_renovated'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.265852Z","iopub.execute_input":"2022-03-24T13:52:37.266541Z","iopub.status.idle":"2022-03-24T13:52:37.277851Z","shell.execute_reply.started":"2022-03-24T13:52:37.266489Z","shell.execute_reply":"2022-03-24T13:52:37.276911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This time let's use sklearn to build our model.\n\nreg = LinearRegression()\nreg.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.27996Z","iopub.execute_input":"2022-03-24T13:52:37.28062Z","iopub.status.idle":"2022-03-24T13:52:37.321981Z","shell.execute_reply.started":"2022-03-24T13:52:37.280567Z","shell.execute_reply":"2022-03-24T13:52:37.320421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the assumption of normality. \n# It seems quite good, right :)\n\ny_hat = reg.predict(x_train)\n\nsns.histplot(y_train - y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.324165Z","iopub.execute_input":"2022-03-24T13:52:37.324798Z","iopub.status.idle":"2022-03-24T13:52:37.881861Z","shell.execute_reply.started":"2022-03-24T13:52:37.324753Z","shell.execute_reply":"2022-03-24T13:52:37.880981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To measure adjusted R2, I will write a simple function. (Train)\n\ndef adj_R2(x_train,y_train):\n  r2 = reg.score(x_train,y_train)\n  n = x_train.shape[0]\n  p = x_train.shape[1]\n  adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n  return adjusted_r2\n\nadj_R2 = adj_R2(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.88341Z","iopub.execute_input":"2022-03-24T13:52:37.883736Z","iopub.status.idle":"2022-03-24T13:52:37.902107Z","shell.execute_reply.started":"2022-03-24T13:52:37.883699Z","shell.execute_reply":"2022-03-24T13:52:37.900999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To measure adjusted R2, I will write a simple function. (Test)\n\ndef adj_R2_test(x_test,y_test):\n  r2 = reg.score(x_test,y_test)\n  n = x_test.shape[0]\n  p = x_test.shape[1]\n  adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n  return adjusted_r2\n\nadj_R2_test = adj_R2_test(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.90379Z","iopub.execute_input":"2022-03-24T13:52:37.904441Z","iopub.status.idle":"2022-03-24T13:52:37.92315Z","shell.execute_reply.started":"2022-03-24T13:52:37.904389Z","shell.execute_reply":"2022-03-24T13:52:37.921936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our training and test scores are very similar,\n# which means that overfitting was not observed in our model. \nprint('R2-Train                      : {0:.2f}'.format(reg.score(x_train, y_train)*100))\nprint('R2-Test                       : {0:.2f}'.format(reg.score(x_test, y_test)*100))\nprint('Adj_R2-Train                  : {0:.2f}'.format(adj_R2*100))\nprint('Adj_R2-Test                   : {0:.2f}'.format(adj_R2_test*100))\nprint('MSE (Mean Squared Error)      : {0:.0f}'.format(metrics.mean_squared_error(y_train, y_hat)))\nprint('RMSE (Root Mean Squared Error): {0:.0f}'.format(np.sqrt(metrics.mean_squared_error(y_train, y_hat))))\nprint('MAE (Mean Ablosute Error)     : {0:.0f}'.format(metrics.mean_absolute_error(y_train, y_hat)))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.93509Z","iopub.execute_input":"2022-03-24T13:52:37.936373Z","iopub.status.idle":"2022-03-24T13:52:37.970648Z","shell.execute_reply.started":"2022-03-24T13:52:37.936297Z","shell.execute_reply":"2022-03-24T13:52:37.96971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's prepare a regression model summary table.\n\n# Weights means for continuous variables: \n# positive weight = shows that as a feature increases in value, so do the price respectively\n# negative weight = shows that as a feature increases in value, price decrease\n\n# Weights means for dummy variables:\n# positive weight = shows that the respective category(city) is more expensive than the benchmark\n# negative weight = shows that the respective category(city) is less expensive than the benchmark \n\n\nreg_summary = pd.DataFrame(x_train.columns.values, columns = ['Features'])\nreg_summary['Weights'] = reg.coef_.reshape(-1,1)\nreg_summary.index = reg_summary.index +1\nreg_summary.loc[0] = ['Intercept (b0)', reg.intercept_[0]]\nreg_summary = reg_summary.sort_index()\nreg_summary\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:52:37.972489Z","iopub.execute_input":"2022-03-24T13:52:37.97317Z","iopub.status.idle":"2022-03-24T13:52:38.015299Z","shell.execute_reply.started":"2022-03-24T13:52:37.973119Z","shell.execute_reply":"2022-03-24T13:52:38.014223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model has reached an acceptable point. Our next step should be to analyze our erroneous predictions in detail and work on them to improve our predictions. \n\nI hope that you find it useful. As I will be happy to improve myself, your comments and feedbacks are always welcome, as are suggestions for additional information that could usefully be included. Thank you! \n\n","metadata":{}}]}